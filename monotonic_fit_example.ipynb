{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rhys M. Adams\n",
    "\n",
    "Let's say you have a set of amino acid sequences ($\\bf s$) and some sort of phenotype ($f$) associated with each sequence. You want to predict the phenotype of unknown sequences. One way to do this is by adding up the contributions of single mutations via a Position Weight Matrix (PWM) model (e.g. Stormo, Bioinformatics 2000). What exactly should you add up? In this notebook, I demonstrate a method, monotonic_fit, that transforms the data to maximize PWM prediction to actual values.\n",
    "\n",
    "Let's get all of the imports out of the way and initialize a few generic constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from scipy.stats import linregress\n",
    "#The following two are custom libraries\n",
    "#makeSparse converts integer sequences into a sparse array that allows for quick linear algebra\n",
    "from make_A import makeSparse\n",
    "#monotonic fit tries to find a monotonic, non-linear transformation that yields the most additive PWM\n",
    "from monotonic_fit import monotonic_fit\n",
    "np.random.seed(0)\n",
    "pylab.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample all sequences with single mutations, and then randomly sample sequences with an average of 2 mutations (seq). Finally we convert the sequences (seq) to a sparse matrix, A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seq = 100000\n",
    "mut_rate = 2\n",
    "seq_length = 20\n",
    "alphabet_size = 20 #i.e. amino acids\n",
    "num_muts = np.random.poisson(mut_rate, num_seq)\n",
    "wt_seq = np.zeros(seq_length)\n",
    "seq = []\n",
    "for ii in range(seq_length):\n",
    "    for jj in range(1, alphabet_size):\n",
    "        curr = np.zeros(seq_length)\n",
    "        curr[ii] = jj\n",
    "        seq.append(curr)\n",
    "\n",
    "for muts in num_muts:\n",
    "    curr = np.zeros(seq_length)\n",
    "    mut_pos = np.random.permutation(seq_length)[:muts]\n",
    "    curr[mut_pos] = np.random.randint(1,alphabet_size)\n",
    "    seq.append(curr)\n",
    "\n",
    "_, A = makeSparse(seq, wt_seq, alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assign an energy to each mutant, and list them as a vector (x), we can quickly estimate the total energy by calculating $\\bf E = \\bf A\\cdot x$. Let's generate some random PWM \"energies\" (x) for our sequences, and calculate their total energies (E), and then add some noise (epsilon). Finally, we simulate our measurement boundaries (lims). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_strength = 1./2\n",
    "x = np.random.randn(A.shape[1])\n",
    "E_original = A.dot(x)\n",
    "epsilon = np.random.randn(E_original.shape[0]) * np.std(E_original) * noise_strength\n",
    "E = E_original + epsilon\n",
    "lims = np.array([np.sort(E)[2000], np.sort(E)[-2000]]) #this defines boundary methods. \n",
    "\n",
    "E_scale = lims[1]-lims[0]\n",
    "rel_E = E / E_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert energies to some measurable phenotype (f). In this case, I use sin to represent the distortion that might arise from experimental error in measuring a phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_f(transform, lims):\n",
    "    f_lims = np.sort(transform(lims))\n",
    "    f = transform(E)\n",
    "    f[f<f_lims[0]] = f_lims[0]\n",
    "    f[f>f_lims[1]] = f_lims[1]\n",
    "    return f, f_lims\n",
    "\n",
    "g_inv = lambda x:np.sin(2*x) + 2*x\n",
    "f, f_lims = make_f(g_inv, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf \\text{Problem definition}$ At this point we have all of the variables we need to define the problem: ${\\bf f} = g^{-1}({\\bf E})$, and ${\\bf E} = {\\bf A \\cdot x}$. The question fundamentally is this: given that ${\\bf f}$ and ${\\bf A}$ are known, can we infer a monotonic function $g$ that gives us the original energies ${\\bf E}$? I set up this problem as\n",
    "\n",
    "argmin$_g (g({\\bf f}) - {\\bf A} \\cdot g({\\bf f_1}))^T\\cdot (g({\\bf f}) - {\\bf A} \\cdot g({\\bf f_1})) + \\alpha \\int \\left (\\frac{\\partial^2 g(f)}{\\partial f^2}\\right )^2 df$\n",
    "\n",
    "where ${\\bf f_1}$ is the set of measurements for the single mutants corresponding to $\\bf x$, and the fit is subject to constraints \n",
    "\n",
    "$g_{max} - g_{min} = 1$, $g$ has a range of 1\n",
    "\n",
    "$g(f_{wt}) = 0$, the reference sequence has an energy of 0\n",
    "\n",
    "$g(f+|c|) - g(f) >0 $, $g$ is monotonic\n",
    "\n",
    "and the constraint that the PWM cannot exceed $g_{min}$ and $g_{max}$,\n",
    "$g_{min} \\le {\\bf A_i} \\cdot g({\\bf f_1}) \\le g_{max}$\n",
    "\n",
    "Back to the coding, I define a helper function plotting hyper-parameter scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scan(alphas, Rsquare, ax, full_height=True):\n",
    "    ax.semilogx(alphas, Rsquare)\n",
    "    ax.axhline(1-objective[-1], c=[0.3,0.3,0.3])\n",
    "    best_ind = np.argsort(Rsquare)[-1]\n",
    "    ax.scatter(alphas[best_ind], Rsquare[best_ind],s=100, c=[1,0,0])\n",
    "    ax.set_xlim([np.min(alphas), np.max(alphas)])\n",
    "    ax.text(0.05,0.1,r'best $R^2$=%.3f'%(Rsquare[best_ind]), transform=ax.transAxes)\n",
    "    if full_height:\n",
    "        ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I sample different numbers of sequences (i.e. $10^3$, $10^4$ and all sequences), and ask how this affects the monotonic fits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 36\n",
    "alphas = np.logspace(-8, 6, num_points)\n",
    "\n",
    "for sub_sample in [1000, 10000, A.shape[0]]:\n",
    "    #sub_sample the unknown\n",
    "    sub_A = A[:sub_sample]\n",
    "    sub_f = f[:sub_sample]\n",
    "    #sub_sample the original verification variables\n",
    "    sub_E = rel_E[:sub_sample]\n",
    "    \n",
    "    #fit for Ehat = g(f)\n",
    "    _, Ehat, alphas, objective = monotonic_fit(sub_A, sub_f, f_lims, alphas, name='Jupyter_sin_example', already_fit=False, suppress_out=True, random_seed=0)\n",
    "    #I used 1-R^2 as the objective\n",
    "    Rsquare = 1 - objective\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2,figsize=(10,3.5))\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    ax = axes[0]\n",
    "    \n",
    "    #plot the scan for alpha\n",
    "    plot_scan(alphas, Rsquare, ax, full_height=False)\n",
    "    ax.set_ylabel(r'$R^2$')\n",
    "    ax.set_xlabel(r'$\\alpha$, penalty')\n",
    "    ax.set_title(r'$\\alpha$ scan, # sampled: %i'%sub_sample)\n",
    "\n",
    "    #rescale Ehat so that it is roughly the same scale as E\n",
    "    actually_fit = (sub_f > -8) & (sub_f < 8)\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(Ehat[actually_fit],sub_E[actually_fit])\n",
    "    Ehat = Ehat * slope + intercept\n",
    "\n",
    "    #compare E to Ehat\n",
    "    ax = axes[1]\n",
    "    ind = np.argsort(sub_E)\n",
    "    ax.plot(sub_f[ind], sub_E[ind], label='actual')\n",
    "    ax.plot(sub_f[ind], Ehat[ind], label='fit')\n",
    "    \n",
    "    #frou frou plotting commands\n",
    "    ax.set_xlabel(r'$f$')\n",
    "    ax.set_ylabel(r'$E$')\n",
    "    ax.set_title('fit v actual')\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    ax.set_yscale('symlog', linthreshy=1, linscaley=4)\n",
    "    ax.set_yticks([-10,-1,-0.5,0,0.5,1,10])\n",
    "    ax.set_yticklabels(['-10','-1','-0.5','0','0.5','1','10'])\n",
    "    ax.set_xticks([-10,0,10])\n",
    "    ax.set_xlim([-10,10])\n",
    "    \n",
    "    #calculate the L2 distance between E and Ehat\n",
    "    deviation = np.trapz((np.sort(sub_E[actually_fit]) - np.sort(Ehat[actually_fit]))**2, np.sort(sub_f[actually_fit]))\n",
    "    ax.text(0.1,0.1,r'distance=%.3f'%(deviation), transform=ax.transAxes)\n",
    "\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ were calculated using 10 fold cross-validation. The fit $E$ were rescaled, so that the range is no longer 1, but this scale was arbitrary and unknown, so it should only help align the fit $\\hat{E}$ to the actual $E$. Increasing the sampling rate improves the monotonic fit. As sampling increased the real answer was eventually found. This example is meant to show the limitations of this approach, so the fits only partially give a good answer until all sequences are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
